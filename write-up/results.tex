\subsection{Unsupervised Learning}
There were no unsupervised methods that provided obvious structure  that matched the classifications.  For the K-means method, a variety of k-values were used.  In each case, the positive cases were divided among the categories in rough proportion to the total number of cases in each category.  A similar thing happened with LDA.  Since LDA is a mixed membership model, we decided to assign hard membership if an object had larger than a certain threshold (e.g., 0.8 or 0.9) membership in a particular topic.  Members of a particular topic were distributed between positive and negative examples in rough proportion to the actual distributions of the examples in the data set.  GMM, for small values of \texttt{n\_components}, was able to do better than just randomly pulling from the dataset for its category assignment.  For example, for \texttt{n\_components} = 4, one of the categories (using a posterior probability threshold of 0.9) had ${\sim}$25\% of its members being positive examples and another category had only ${\sim}$0.1\% of its members being positive examples.  If the GMM category assignment was nothing more than random draws from the data, then both of these percentages would be around 3.3\%.  (The other two categories have ${\sim}$1--2\% of their members being positive examples).  Thus, GMM was able to uncover some features in the data that allowed for a small differentiation between positive and negative examples.  Unfortunately, only ${\sim}$10\% of the positive examples end up in the positive-heavy category, which category itself still consists ${\sim}$75\% of negative examples, so the GMM categories themselves are not sufficiently discriminatory to be used as our model.
For PCA, the first principal component explains 20\% of the variance, then 11\%, 8.5\%, and 7.8\% for the next three principal components.  Figure~\ref{pca} shows the projection of the data onto the first two principal components.  The green circles are the positive cases and blue dots are the negative cases.  From just the first two principal components, there is no exploitable separation between the positive and negative cases that allows for classification.  Running K-means (with several different values of k) on the PCA decomposed values also revelead no exploitable structure for classification.
\begin{figure}
\begin{centering}
\includegraphics[width=5in]{pca.png}
\caption{\label{pca} The data projected on the first two principal components.  The positive cases are in green and the negative cases are in blue.  Note that the scales of the axes are the same; they have been stretched and squished to better fit in the single-column format of this paper.  The transparency of the green points is less than the blue points so that they could be better visible, since there are about 30 times more blue points than green points.  There is no obvious separation between the two groups of points, though the green points do have larger variance than the blue points along the first principal component (22.6 versus 17.7).  In the axis labels, ``PC'' = principal component.}
\end{centering}
\end{figure}







\subsection{Supervised Learning}