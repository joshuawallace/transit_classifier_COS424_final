Despite the presence of different physical effects in our data, we were unable to train an unsupervised classifier to disentangle these effects. It is likely that these physical processes appear too similar in our current data set. The unsupervised methods still provided an interesting exploration of the data. The supervised methods provided significantly better results with all of our classifiers vastly outperformed random guessing. The best methods were the Naive Bayes and the Gradient boosted method, a priori we expected the AdaBoosting method to perform best and are unsure why this did not perform better.

Our classification problem is quite difficult as the boundary between accepted and rejected candidates is hard to define with many of the accepted cases being difficult, even for a human, to distinguish. Thus we were happy with the levels of performance obtained by our classifiers, though further improvement is still desired. We hope that by performing more careful feature engineering and by including the images, which our features were derived from, that we can improve further. Due to time limitations we were unable to fully study the effect of changing the probability to accept and reject candidates. It would interesting to train our models to find the best cutoff probability, this could be treated as a hyperparameter and selected by cross-validation.


In future work we would like to more carefully examine how to weight the classifiers when combining them; currently all the classifiers have equal weight in the voting, despite the fact that some of the classifiers work much better than the others. Beyond this we plan to incorporate these classifiers in a statistical model to estimate the occurrence of hot Jupiters, a step that had been impossible for the HAT data with human selection of candidates.