\subsection{Unsupervised Learning}
We used some unsupervised learning to see if any structure that arises in the models aligns well with the classification.  We use the K-means, principal component analysis (PCA), Gaussian mixture model (GMM), and latent Dirichlet allocation (LDA) algorithms from \texttt{scikit learn}\cite{scikit-learn}.  Data are scaled prior to the PCA analysis, and features are re-centered to avoid negative values for the LDA algorithm.  Clusters and cluster membership are examined to see if any clusters have predominately and most of the positive examples. We also use the PCA-transformed data in supervised learning models.




\subsection{Supervised Learning}
We used several supervised classifiers to attempt to predict the class label for our data set. We consider the following classfiers from \texttt{scikit learn}\cite{scikit-learn}: logistic regression (with l1 and l2 penalties), naive bayes, SVM (with polynomail and RBF kernels) and four variations on decision trees; random forest, extra-trees, gradient boosted trees and adaboost trees. We used mutual inforamation to select the best n-features, where n was choosen by cross-validation. For all the methods with hyperparameters, we fit them using cross validation. Finally we combined all of these classifiers together and used them in a combined manner. For the combined classifier we used two methods, first where the classifiers 'voted' on the label with the label set by the majority vote. In the second method we weighted the predicted labels by the classifier's prediction probability.





\subsection{Focus on One Algorithm: }